# ============================================
# 1. Imports and Setup
# ============================================

# Import necessary standard libraries
import os  # For accessing environment variables securely
import time  # For measuring latency
import logging  # For logging decisions, errors, and actions
import requests  # For making HTTP requests to external APIs
from enum import Enum  # For creating enumerated constants
from typing import Any, Dict, List, Literal, Optional, Protocol, Union  # For type hinting
from dataclasses import dataclass  # For creating simple data classes

# Import Pydantic for data validation and settings management
from pydantic import BaseModel, ConfigDict, Field

# Import Annotated for advanced type hinting from typing_extensions
from typing_extensions import Annotated

# Import decorators from schema_utils within llama_models
from llama_models.schema_utils import json_schema_type, webmethod

# Import all datatypes from llama3.api.datatypes, ignoring flake8 F403 warnings
from llama_models.llama3.api.datatypes import *  # noqa: F403

# Import all deployment types from llama_stack.apis.common.deployment_types, ignoring flake8 F403 warnings
from llama_stack.apis.common.deployment_types import *  # noqa: F403

# Import all inference-related classes and functions from llama_stack.apis.inference, ignoring flake8 F403 warnings
from llama_stack.apis.inference import *  # noqa: F403

# Import all safety-related classes and functions from llama_stack.apis.safety, ignoring flake8 F403 warnings
from llama_stack.apis.safety import *  # noqa: F403

# Import all memory-related classes and functions from llama_stack.apis.memory, ignoring flake8 F403 warnings
from llama_stack.apis.memory import *  # noqa: F403

# ============================================
# 2. Data Models and Enums
# ============================================

# Attachment class representing an attachment containing content and MIME type
@json_schema_type  # Decorator to define JSON schema type for the Attachment class
class Attachment(BaseModel):
    """
    Represents an attachment that can be part of an agent's interaction.

    Attributes:
        content (InterleavedTextMedia | URL): The content of the attachment.
        mime_type (str): The MIME type of the attachment content.
    """
    # The content of the attachment, which can be either InterleavedTextMedia or a URL
    content: InterleavedTextMedia | URL
    # The MIME type of the attachment content as a string
    mime_type: str

# Enum to define different types of agent tools available
class AgentTool(Enum):
    """
    Enum for the different tools that an agent can utilize.

    Attributes:
        brave_search: Represents the Brave search tool.
        wolfram_alpha: Represents the Wolfram Alpha tool.
        photogen: Represents the Photogen tool.
        code_interpreter: Represents a code interpreter tool.
        function_call: Represents a function calling tool.
        memory: Represents a memory tool.
        openai_gpt: Represents the OpenAI GPT tool.
        llama: Represents the Llama tool.
    """
    # Represents the Brave search tool
    brave_search = "brave_search"
    # Represents the Wolfram Alpha tool
    wolfram_alpha = "wolfram_alpha"
    # Represents the Photogen tool
    photogen = "photogen"
    # Represents a code interpreter tool
    code_interpreter = "code_interpreter"
    # Represents a function calling tool
    function_call = "function_call"
    # Represents a memory tool
    memory = "memory"
    # Represents the OpenAI GPT tool
    openai_gpt = "openai_gpt"
    # Represents the Llama tool
    llama = "llama"

# Tool definition with common fields
class ToolDefinitionCommon(BaseModel):
    """
    Common attributes for all tool definitions.

    Attributes:
        input_shields (Optional[List[str]]): A list of input shields used for filtering input data.
        output_shields (Optional[List[str]]): A list of output shields used for filtering output data.
    """
    # Optional list of input shields with a default empty list
    input_shields: Optional[List[str]] = Field(default_factory=list)
    # Optional list of output shields with a default empty list
    output_shields: Optional[List[str]] = Field(default_factory=list)

# Enum to represent different search engines available for use
class SearchEngineType(Enum):
    """
    Enum for the different search engines available for use in the search tool.

    Attributes:
        bing: Represents the Bing search engine.
        brave: Represents the Brave search engine.
    """
    # Represents the Bing search engine
    bing = "bing"
    # Represents the Brave search engine
    brave = "brave"

# Memory bank configurations for agents to store and retrieve memory
class _MemoryBankConfigCommon(BaseModel):
    """
    Common attributes for all memory bank configurations.

    Attributes:
        bank_id (str): The identifier for the memory bank.
    """
    # The unique identifier for the memory bank
    bank_id: str

# Configuration for a vector-based memory bank
class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a vector-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.vector.value]): The type of memory bank (vector).
    """
    # The type of memory bank, fixed to 'vector'
    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value

# Configuration for a key-value-based memory bank
class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a key-value-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.keyvalue.value]): The type of memory bank (key-value).
        keys (List[str]): List of keys to focus on for storing memory.
    """
    # The type of memory bank, fixed to 'keyvalue'
    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value
    # List of keys that the memory bank will focus on for storing memory
    keys: List[str]

# Configuration for a keyword-based memory bank
class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a keyword-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.keyword.value]): The type of memory bank (keyword).
    """
    # The type of memory bank, fixed to 'keyword'
    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value

# Configuration for a graph-based memory bank
class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a graph-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.graph.value]): The type of memory bank (graph).
        entities (List[str]): List of entities to focus on in the memory bank.
    """
    # The type of memory bank, fixed to 'graph'
    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value
    # List of entities that the graph-based memory bank will focus on
    entities: List[str]

# Unified MemoryBankConfig with support for multiple memory bank types
MemoryBankConfig = Annotated[
    Union[
        AgentVectorMemoryBankConfig,          # Vector-based memory bank configuration
        AgentKeyValueMemoryBankConfig,        # Key-value-based memory bank configuration
        AgentKeywordMemoryBankConfig,         # Keyword-based memory bank configuration
        AgentGraphMemoryBankConfig,           # Graph-based memory bank configuration
    ],
    Field(discriminator="type"),  # Use the 'type' field to discriminate between different configurations
]

# Memory query generators used for retrieving memory from memory banks
class MemoryQueryGenerator(Enum):
    """
    Enum for different query generators used for retrieving memory from memory banks.

    Attributes:
        default: Represents the default query generator.
        llm: Represents a query generator based on a language model.
        custom: Represents a custom query generator.
    """
    # Represents the default query generator
    default = "default"
    # Represents a language model-based query generator
    llm = "llm"
    # Represents a custom query generator
    custom = "custom"

# ============================================
# 3. Metrics Tracking and Services
# ============================================

# Dataclass to hold metrics for each tool
@dataclass
class ToolMetrics:
    """
    Data class to store metrics for each tool.

    Attributes:
        cost (float): Cost per request in USD.
        latency (float): Latency in seconds.
        quality (float): Quality score (e.g., 0 to 1).
        co2_impact (float): CO2 impact in kg per request.
    """
    cost: float  # Cost per request in USD
    latency: float  # Latency in seconds
    quality: float  # Quality score (e.g., 0 to 1)
    co2_impact: float  # CO2 impact in kg per request

# Enum to define different types of agent tools available (extended)
class AgentTool(Enum):
    """
    Enum for the different tools that an agent can utilize.

    Attributes:
        brave_search: Represents the Brave search tool.
        wolfram_alpha: Represents the Wolfram Alpha tool.
        photogen: Represents the Photogen tool.
        code_interpreter: Represents a code interpreter tool.
        function_call: Represents a function calling tool.
        memory: Represents a memory tool.
        openai_gpt: Represents the OpenAI GPT tool.
        llama: Represents the Llama tool.
    """
    brave_search = "brave_search"
    wolfram_alpha = "wolfram_alpha"
    photogen = "photogen"
    code_interpreter = "code_interpreter"
    function_call = "function_call"
    memory = "memory"
    openai_gpt = "openai_gpt"
    llama = "llama"

# MetricsService class to retrieve and manage tool metrics
class MetricsService:
    """
    Service to retrieve and manage metrics for each tool.

    Attributes:
        base_costs (Dict[AgentTool, float]): Base cost per request for each tool.
        base_co2 (Dict[AgentTool, float]): Base CO2 impact per request for each tool.
    """
    def __init__(self):
        # Initialize base costs for tools in USD
        self.base_costs: Dict[AgentTool, float] = {
            AgentTool.brave_search: 0.01,
            AgentTool.wolfram_alpha: 0.02,
            AgentTool.photogen: 0.015,
            AgentTool.code_interpreter: 0.025,
            AgentTool.function_call: 0.02,
            AgentTool.memory: 0.005,
            AgentTool.openai_gpt: 0.03,
            AgentTool.llama: 0.025,
        }
        # Initialize base CO2 impacts for tools in kg
        self.base_co2: Dict[AgentTool, float] = {
            AgentTool.brave_search: 0.0005,
            AgentTool.wolfram_alpha: 0.0007,
            AgentTool.photogen: 0.0006,
            AgentTool.code_interpreter: 0.001,
            AgentTool.function_call: 0.0008,
            AgentTool.memory: 0.0003,
            AgentTool.openai_gpt: 0.0006,
            AgentTool.llama: 0.00055,
        }

    def get_metrics(self, tool: AgentTool) -> ToolMetrics:
        """
        Retrieve metrics for a given tool, dynamically measuring latency.

        Args:
            tool (AgentTool): The tool for which to retrieve metrics.

        Returns:
            ToolMetrics: The metrics associated with the tool.
        """
        if tool == AgentTool.openai_gpt:
            return self.get_openai_metrics()
        elif tool == AgentTool.llama:
            return self.get_llama_metrics()
        else:
            # Return static metrics for other tools
            return ToolMetrics(
                cost=self.base_costs.get(tool, float('inf')),
                latency=0.5,  # Placeholder latency
                quality=0.8,  # Placeholder quality
                co2_impact=self.base_co2.get(tool, float('inf'))
            )

    def get_openai_metrics(self) -> ToolMetrics:
        """
        Dynamically retrieve metrics for the OpenAI GPT tool.

        Returns:
            ToolMetrics: The dynamically measured metrics for OpenAI GPT.
        """
        start_time = time.time()  # Record the start time for latency measurement
        try:
            # Make a lightweight request to OpenAI to measure actual latency
            response = requests.get(
                "https://api.openai.com/v1/models",
                headers={
                    "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"
                },
                timeout=5  # Set a timeout to prevent hanging
            )
            latency = time.time() - start_time  # Calculate latency
            if response.status_code == 200:
                # Estimate cost based on a predefined base cost
                cost = self.base_costs[AgentTool.openai_gpt]
                quality = 0.95  # Example quality score
                co2 = self.base_co2[AgentTool.openai_gpt]
                return ToolMetrics(cost=cost, latency=latency, quality=quality, co2_impact=co2)
            else:
                # Handle unexpected status codes by returning worst-case metrics
                return ToolMetrics(cost=float('inf'), latency=latency, quality=0, co2_impact=float('inf'))
        except requests.RequestException:
            # In case of request failure, return worst-case metrics
            return ToolMetrics(cost=float('inf'), latency=float('inf'), quality=0, co2_impact=float('inf'))

    def get_llama_metrics(self) -> ToolMetrics:
        """
        Dynamically retrieve metrics for the Llama tool.

        Returns:
            ToolMetrics: The dynamically measured metrics for Llama.
        """
        start_time = time.time()  # Record the start time for latency measurement
        try:
            # Hypothetical Llama API endpoint to check status or measure latency
            response = requests.get(
                "https://api.llama.ai/v1/status",
                headers={
                    "Authorization": f"Bearer {os.getenv('LLAMA_API_KEY')}"
                },
                timeout=5  # Set a timeout to prevent hanging
            )
            latency = time.time() - start_time  # Calculate latency
            if response.status_code == 200:
                # Estimate cost based on a predefined base cost
                cost = self.base_costs[AgentTool.llama]
                quality = 0.9  # Example quality score
                co2 = self.base_co2[AgentTool.llama]
                return ToolMetrics(cost=cost, latency=latency, quality=quality, co2_impact=co2)
            else:
                # Handle unexpected status codes by returning worst-case metrics
                return ToolMetrics(cost=float('inf'), latency=latency, quality=0, co2_impact=float('inf'))
        except requests.RequestException:
            # In case of request failure, return worst-case metrics
            return ToolMetrics(cost=float('inf'), latency=float('inf'), quality=0, co2_impact=float('inf'))

    def update_metrics(self, tool: AgentTool, metrics: ToolMetrics):
        """
        Update the metrics for a given tool. This can be used if metrics are dynamically changing.

        Args:
            tool (AgentTool): The tool for which to update metrics.
            metrics (ToolMetrics): The new metrics to assign to the tool.
        """
        # Update the base costs and CO2 impacts if needed
        self.base_costs[tool] = metrics.cost
        self.base_co2[tool] = metrics.co2_impact

# ============================================
# 4. Scalability with Plugin Architecture
# ============================================

# Protocol for tool plugins to ensure consistency
class ToolPlugin(Protocol):
    """
    Protocol that all tool plugins must adhere to.

    Methods:
        execute(prompt: str) -> CompletionMessage: Execute the tool with the given prompt.
    """
    def execute(self, prompt: str) -> CompletionMessage:
        ...

# Plugin registry to manage tool plugins dynamically
class PluginRegistry:
    """
    Registry to manage and retrieve tool plugins dynamically.

    Attributes:
        _plugins (Dict[AgentTool, ToolPlugin]): Mapping from AgentTool to their respective plugins.
    """
    def __init__(self):
        # Initialize an empty dictionary to hold plugins
        self._plugins: Dict[AgentTool, ToolPlugin] = {}

    def register_plugin(self, tool: AgentTool, plugin: ToolPlugin):
        """
        Register a new tool plugin.

        Args:
            tool (AgentTool): The tool to register.
            plugin (ToolPlugin): The plugin instance associated with the tool.
        """
        self._plugins[tool] = plugin

    def get_plugin(self, tool: AgentTool) -> ToolPlugin:
        """
        Retrieve the plugin associated with a given tool.

        Args:
            tool (AgentTool): The tool for which to retrieve the plugin.

        Returns:
            ToolPlugin: The plugin associated with the tool.

        Raises:
            ValueError: If the tool is not registered.
        """
        if tool in self._plugins:
            return self._plugins[tool]
        else:
            raise ValueError(f"Plugin for tool '{tool.value}' is not registered.")

# ============================================
# 5. Tool Clients (OpenAI and Llama)
# ============================================

# OpenAIClient class to interact with OpenAI APIs
class OpenAIClient:
    """
    Client to interact with OpenAI APIs.

    Attributes:
        api_key (str): API key for authenticating with OpenAI.
        model (str): The OpenAI model to use (e.g., 'davinci').
    """
    def __init__(self, api_key: str, model: str = "davinci"):
        """
        Initialize the OpenAIClient with the given API key and model.

        Args:
            api_key (str): API key for OpenAI.
            model (str, optional): The OpenAI model to use. Defaults to "davinci".
        """
        self.api_key = api_key  # Store the API key
        self.model = model  # Store the model name

    def execute(self, prompt: str) -> CompletionMessage:
        """
        Execute a prompt using the OpenAI API and return the completion message.

        Args:
            prompt (str): The prompt to send to the OpenAI model.

        Returns:
            CompletionMessage: The response from the OpenAI model.

        Raises:
            requests.RequestException: If the API request fails.
        """
        # Define the endpoint URL for OpenAI completions
        url = "https://api.openai.com/v1/completions"
        # Define the headers with the API key for authorization
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        # Define the payload with the model and prompt
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_tokens": 150
        }
        # Send the POST request to OpenAI
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        # Raise an exception if the request was unsuccessful
        response.raise_for_status()
        # Parse the response JSON to extract the generated text
        generated_text = response.json()["choices"][0]["text"].strip()
        # Return the completion message
        return CompletionMessage(content=generated_text)

# LlamaClient class to interact with Llama APIs
class LlamaClient:
    """
    Client to interact with Llama APIs.

    Attributes:
        api_key (str): API key for authenticating with Llama.
        model (str): The Llama model to use (e.g., 'llama-2').
    """
    def __init__(self, api_key: str, model: str = "llama-2"):
        """
        Initialize the LlamaClient with the given API key and model.

        Args:
            api_key (str): API key for Llama.
            model (str, optional): The Llama model to use. Defaults to "llama-2".
        """
        self.api_key = api_key  # Store the API key
        self.model = model  # Store the model name

    def execute(self, prompt: str) -> CompletionMessage:
        """
        Execute a prompt using the Llama API and return the completion message.

        Args:
            prompt (str): The prompt to send to the Llama model.

        Returns:
            CompletionMessage: The response from the Llama model.

        Raises:
            requests.RequestException: If the API request fails.
        """
        # Define the endpoint URL for Llama completions
        url = f"https://api.llama.ai/v1/models/{self.model}/completions"
        # Define the headers with the API key for authorization
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        # Define the payload with the prompt
        payload = {
            "prompt": prompt,
            "max_tokens": 150
        }
        # Send the POST request to Llama
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        # Raise an exception if the request was unsuccessful
        response.raise_for_status()
        # Parse the response JSON to extract the generated text
        generated_text = response.json()["choices"][0]["text"].strip()
        # Return the completion message
        return CompletionMessage(content=generated_text)

# ============================================
# 6. Tool Selection Logic
# ============================================

# Enum to define selection modes
class SelectionMode(str, Enum):
    """
    Enum to define the selection mode for tool assignment.

    Attributes:
        manual: Developer manually selects the tool.
        automated: The system automatically selects the best tool based on metrics.
    """
    manual = "manual"
    automated = "automated"

# AgentConfiguration class to manage selection mode
class AgentConfiguration(BaseModel):
    """
    Configuration model to manage the tool selection mode.

    Attributes:
        selection_mode (SelectionMode): The mode of tool selection (manual or automated).
    """
    # The selection mode, defaulting to manual
    selection_mode: SelectionMode = SelectionMode.manual

# ToolSelector class to handle tool selection based on the current mode
class ToolSelector:
    """
    Class to handle the selection of tools based on the current configuration mode.

    Attributes:
        metrics_service (MetricsService): Service to retrieve tool metrics.
        registry (PluginRegistry): Registry to manage tool plugins.
    """
    def __init__(self, metrics_service: MetricsService, registry: PluginRegistry):
        """
        Initialize the ToolSelector with the given MetricsService and PluginRegistry.

        Args:
            metrics_service (MetricsService): Service to retrieve tool metrics.
            registry (PluginRegistry): Registry to manage tool plugins.
        """
        self.metrics_service = metrics_service  # Assign the metrics service
        self.registry = registry  # Assign the plugin registry

    def select_tool_manual(self, available_tools: List[AgentTool]) -> AgentTool:
        """
        Allow the developer to manually select a tool by displaying metrics.

        Args:
            available_tools (List[AgentTool]): List of available tools to choose from.

        Returns:
            AgentTool: The selected tool.

        Raises:
            ValueError: If the selected tool name is invalid.
        """
        # Display tool metrics to the developer
        for tool in available_tools:
            metrics = self.metrics_service.get_metrics(tool)  # Retrieve metrics for the tool
            print(f"Tool: {tool.value}")  # Print the tool name
            print(f"  Cost: ${metrics.cost}")  # Print the cost
            print(f"  Latency: {metrics.latency:.2f}s")  # Print the latency
            print(f"  Quality: {metrics.quality}")  # Print the quality score
            print(f"  CO2 Impact: {metrics.co2_impact}kg\n")  # Print the CO2 impact

        # Prompt the developer to select a tool by typing its name
        while True:
            selected_tool = input("Select a tool by typing its name: ").strip().lower()
            # Check if the input matches any available tool
            matched_tools = [tool for tool in available_tools if tool.value == selected_tool]
            if matched_tools:
                return matched_tools[0]  # Return the matched tool
            else:
                print("Invalid tool name. Please try again.")  # Inform the developer of invalid input

    def select_tool_automated(self, available_tools: List[AgentTool]) -> AgentTool:
        """
        Automatically select the best tool based on metrics.

        Args:
            available_tools (List[AgentTool]): List of available tools to choose from.

        Returns:
            AgentTool: The selected tool.
        """
        best_tool = None  # Initialize the best tool
        best_score = float('inf')  # Initialize the best score to infinity

        # Iterate through each available tool to evaluate its score
        for tool in available_tools:
            metrics = self.metrics_service.get_metrics(tool)  # Retrieve metrics for the tool
            # Define a scoring function: minimize cost and CO2 impact, maximize quality
            # Example: weighted sum where cost and CO2 are minimized and quality is maximized
            score = metrics.cost + metrics.co2_impact - (metrics.quality * 0.5)
            if score < best_score:
                best_score = score  # Update the best score
                best_tool = tool  # Update the best tool

        if best_tool:
            self.log_selection(best_tool)  # Log the selected tool
            return best_tool  # Return the selected tool
        else:
            raise ValueError("No suitable tool found for automated selection.")  # Raise an error if no tool is found

    def log_selection(self, tool: AgentTool):
        """
        Log the tool selection decision.

        Args:
            tool (AgentTool): The tool that was selected.
        """
        logging.info(f"Automated selection: {tool.value}")  # Log the selection at INFO level

# ============================================
# 1. Imports and Setup
# ============================================

# Import necessary standard libraries
import os  # For accessing environment variables securely
import time  # For measuring latency
import logging  # For logging decisions, errors, and actions
import requests  # For making HTTP requests to external APIs
from enum import Enum  # For creating enumerated constants
from typing import Any, Dict, List, Literal, Optional, Protocol, Union  # For type hinting
from dataclasses import dataclass  # For creating simple data classes

# Import Pydantic for data validation and settings management
from pydantic import BaseModel, ConfigDict, Field

# Import Annotated for advanced type hinting from typing_extensions
from typing_extensions import Annotated

# Import decorators from schema_utils within llama_models
from llama_models.schema_utils import json_schema_type, webmethod

# Import all datatypes from llama3.api.datatypes, ignoring flake8 F403 warnings
from llama_models.llama3.api.datatypes import *  # noqa: F403

# Import all deployment types from llama_stack.apis.common.deployment_types, ignoring flake8 F403 warnings
from llama_stack.apis.common.deployment_types import *  # noqa: F403

# Import all inference-related classes and functions from llama_stack.apis.inference, ignoring flake8 F403 warnings
from llama_stack.apis.inference import *  # noqa: F403

# Import all safety-related classes and functions from llama_stack.apis.safety, ignoring flake8 F403 warnings
from llama_stack.apis.safety import *  # noqa: F403

# Import all memory-related classes and functions from llama_stack.apis.memory, ignoring flake8 F403 warnings
from llama_stack.apis.memory import *  # noqa: F403

# ============================================
# 2. Data Models and Enums
# ============================================

# Attachment class representing an attachment containing content and MIME type
@json_schema_type  # Decorator to define JSON schema type for the Attachment class
class Attachment(BaseModel):
    """
    Represents an attachment that can be part of an agent's interaction.

    Attributes:
        content (InterleavedTextMedia | URL): The content of the attachment.
        mime_type (str): The MIME type of the attachment content.
    """
    # The content of the attachment, which can be either InterleavedTextMedia or a URL
    content: InterleavedTextMedia | URL
    # The MIME type of the attachment content as a string
    mime_type: str

# Enum to define different types of agent tools available
class AgentTool(Enum):
    """
    Enum for the different tools that an agent can utilize.

    Attributes:
        brave_search: Represents the Brave search tool.
        wolfram_alpha: Represents the Wolfram Alpha tool.
        photogen: Represents the Photogen tool.
        code_interpreter: Represents a code interpreter tool.
        function_call: Represents a function calling tool.
        memory: Represents a memory tool.
        openai_gpt: Represents the OpenAI GPT tool.
        llama: Represents the Llama tool.
    """
    # Represents the Brave search tool
    brave_search = "brave_search"
    # Represents the Wolfram Alpha tool
    wolfram_alpha = "wolfram_alpha"
    # Represents the Photogen tool
    photogen = "photogen"
    # Represents a code interpreter tool
    code_interpreter = "code_interpreter"
    # Represents a function calling tool
    function_call = "function_call"
    # Represents a memory tool
    memory = "memory"
    # Represents the OpenAI GPT tool
    openai_gpt = "openai_gpt"
    # Represents the Llama tool
    llama = "llama"

# Tool definition with common fields
class ToolDefinitionCommon(BaseModel):
    """
    Common attributes for all tool definitions.

    Attributes:
        input_shields (Optional[List[str]]): A list of input shields used for filtering input data.
        output_shields (Optional[List[str]]): A list of output shields used for filtering output data.
    """
    # Optional list of input shields with a default empty list
    input_shields: Optional[List[str]] = Field(default_factory=list)
    # Optional list of output shields with a default empty list
    output_shields: Optional[List[str]] = Field(default_factory=list)

# Enum to represent different search engines available for use
class SearchEngineType(Enum):
    """
    Enum for the different search engines available for use in the search tool.

    Attributes:
        bing: Represents the Bing search engine.
        brave: Represents the Brave search engine.
    """
    # Represents the Bing search engine
    bing = "bing"
    # Represents the Brave search engine
    brave = "brave"

# Memory bank configurations for agents to store and retrieve memory
class _MemoryBankConfigCommon(BaseModel):
    """
    Common attributes for all memory bank configurations.

    Attributes:
        bank_id (str): The identifier for the memory bank.
    """
    # The unique identifier for the memory bank
    bank_id: str

# Configuration for a vector-based memory bank
class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a vector-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.vector.value]): The type of memory bank (vector).
    """
    # The type of memory bank, fixed to 'vector'
    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value

# Configuration for a key-value-based memory bank
class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a key-value-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.keyvalue.value]): The type of memory bank (key-value).
        keys (List[str]): List of keys to focus on for storing memory.
    """
    # The type of memory bank, fixed to 'keyvalue'
    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value
    # List of keys that the memory bank will focus on for storing memory
    keys: List[str]

# Configuration for a keyword-based memory bank
class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a keyword-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.keyword.value]): The type of memory bank (keyword).
    """
    # The type of memory bank, fixed to 'keyword'
    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value

# Configuration for a graph-based memory bank
class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):
    """
    Configuration for a graph-based memory bank.

    Attributes:
        type (Literal[MemoryBankType.graph.value]): The type of memory bank (graph).
        entities (List[str]): List of entities to focus on in the memory bank.
    """
    # The type of memory bank, fixed to 'graph'
    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value
    # List of entities that the graph-based memory bank will focus on
    entities: List[str]

# ============================================
# 3. Metrics Tracking and Services
# ============================================

@dataclass
class ToolMetrics:
    """
    Data class to hold metrics for each tool.

    Attributes:
        cost (float): Cost per request in USD.
        latency (float): Latency in seconds.
        quality (float): Quality score (e.g., 0 to 1).
        co2_impact (float): CO2 impact in kg per request.
    """
    cost: float  # Cost per request in USD
    latency: float  # Latency in seconds
    quality: float  # Quality score (e.g., 0 to 1)
    co2_impact: float  # CO2 impact in kg per request

class MetricsService:
    """
    Service to retrieve and manage metrics for each tool.

    This service can dynamically retrieve metrics such as cost, latency, quality, and CO2 impact.
    """
    def __init__(self):
        # Initialize with base costs and CO2 impacts for tools
        self.base_costs: Dict[AgentTool, float] = {
            AgentTool.openai_gpt: 0.03,
            AgentTool.llama: 0.025,
            AgentTool.brave_search: 0.01,
            AgentTool.wolfram_alpha: 0.02,
            AgentTool.photogen: 0.015,
            AgentTool.code_interpreter: 0.025,
            AgentTool.function_call: 0.02,
            AgentTool.memory: 0.005,
        }
        self.base_co2: Dict[AgentTool, float] = {
            AgentTool.openai_gpt: 0.0006,
            AgentTool.llama: 0.00055,
            AgentTool.brave_search: 0.0005,
            AgentTool.wolfram_alpha: 0.0007,
            AgentTool.photogen: 0.0006,
            AgentTool.code_interpreter: 0.001,
            AgentTool.function_call: 0.0008,
            AgentTool.memory: 0.0003,
        }
        # Configure logging for the MetricsService
        self.logger = logging.getLogger(__name__)
    
    def get_metrics(self, tool: AgentTool) -> ToolMetrics:
        """
        Retrieve the metrics for a given tool, including dynamic latency measurement.

        Args:
            tool (AgentTool): The tool for which to retrieve metrics.

        Returns:
            ToolMetrics: The metrics associated with the tool.
        """
        if tool == AgentTool.openai_gpt:
            return self.get_openai_metrics()
        elif tool == AgentTool.llama:
            return self.get_llama_metrics()
        else:
            # Return static metrics for other tools
            return ToolMetrics(
                cost=self.base_costs.get(tool, float('inf')),
                latency=0.5,  # Default static latency
                quality=0.8,  # Default static quality
                co2_impact=self.base_co2.get(tool, float('inf'))
            )
    
    def get_openai_metrics(self) -> ToolMetrics:
        """
        Dynamically retrieve metrics for the OpenAI GPT tool.

        Returns:
            ToolMetrics: The dynamically retrieved metrics.
        """
        start_time = time.time()  # Start latency measurement
        try:
            # Make a lightweight request to OpenAI to measure latency
            response = requests.get(
                "https://api.openai.com/v1/models",
                headers={
                    "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"
                },
                timeout=5  # Timeout after 5 seconds
            )
            latency = time.time() - start_time  # Calculate latency
            if response.status_code == 200:
                # Estimate cost based on predefined logic
                cost = self.base_costs[AgentTool.openai_gpt]
                quality = 0.95  # Example quality score
                co2 = self.base_co2[AgentTool.openai_gpt]
                return ToolMetrics(cost=cost, latency=latency, quality=quality, co2_impact=co2)
            else:
                # Log unexpected status codes and return worst metrics
                self.logger.error(f"OpenAI API returned status code {response.status_code}")
                return ToolMetrics(cost=float('inf'), latency=latency, quality=0, co2_impact=float('inf'))
        except requests.RequestException as e:
            # Log the exception and return worst metrics
            self.logger.error(f"Error fetching OpenAI metrics: {e}")
            return ToolMetrics(cost=float('inf'), latency=float('inf'), quality=0, co2_impact=float('inf'))
    
    def get_llama_metrics(self) -> ToolMetrics:
        """
        Dynamically retrieve metrics for the Llama tool.

        Returns:
            ToolMetrics: The dynamically retrieved metrics.
        """
        start_time = time.time()  # Start latency measurement
        try:
            # Hypothetical Llama API endpoint to check status or measure latency
            response = requests.get(
                "https://api.llama.ai/v1/status",
                headers={
                    "Authorization": f"Bearer {os.getenv('LLAMA_API_KEY')}"
                },
                timeout=5  # Timeout after 5 seconds
            )
            latency = time.time() - start_time  # Calculate latency
            if response.status_code == 200:
                cost = self.base_costs[AgentTool.llama]
                quality = 0.9  # Example quality score
                co2 = self.base_co2[AgentTool.llama]
                return ToolMetrics(cost=cost, latency=latency, quality=quality, co2_impact=co2)
            else:
                # Log unexpected status codes and return worst metrics
                self.logger.error(f"Llama API returned status code {response.status_code}")
                return ToolMetrics(cost=float('inf'), latency=latency, quality=0, co2_impact=float('inf'))
        except requests.RequestException as e:
            # Log the exception and return worst metrics
            self.logger.error(f"Error fetching Llama metrics: {e}")
            return ToolMetrics(cost=float('inf'), latency=float('inf'), quality=0, co2_impact=float('inf'))
    
    def update_metrics(self, tool: AgentTool, metrics: ToolMetrics):
        """
        Update the metrics for a given tool.

        Args:
            tool (AgentTool): The tool for which to update metrics.
            metrics (ToolMetrics): The new metrics to assign to the tool.
        """
        # This method can be expanded to persist metrics if needed
        self.base_costs[tool] = metrics.cost
        self.base_co2[tool] = metrics.co2_impact
        self.logger.info(f"Updated metrics for {tool.value}: {metrics}")

# ============================================
# 4. Plugin Architecture for Tool Scalability
# ============================================

class ToolPlugin(Protocol):
    """
    Protocol that all tool plugins must adhere to.
    
    Each tool plugin must implement the execute method.
    """
    def execute(self, prompt: str) -> CompletionMessage:
        """
        Execute the tool with the given prompt.

        Args:
            prompt (str): The input prompt for the tool.

        Returns:
            CompletionMessage: The response from the tool.
        """
        ...

# ============================================
# 5. Tool Clients (OpenAI and Llama)
# ============================================

class OpenAIClient(ToolPlugin):
    """
    Client to interact with the OpenAI GPT API.
    """
    def __init__(self, api_key: str):
        """
        Initialize the OpenAIClient with the provided API key.

        Args:
            api_key (str): The API key for OpenAI.
        """
        self.api_key = api_key  # Store the API key securely
        openai.api_key = self.api_key  # Set the API key for OpenAI client
    
    def execute(self, prompt: str) -> CompletionMessage:
        """
        Execute a prompt using the OpenAI GPT model.

        Args:
            prompt (str): The input prompt for the model.

        Returns:
            CompletionMessage: The model's response.
        """
        try:
            start_time = time.time()  # Start latency measurement
            # Make a request to the OpenAI Completion API
            response = openai.Completion.create(
                engine="davinci",  # Specify the model engine
                prompt=prompt,  # Input prompt
                max_tokens=150  # Maximum tokens in the response
            )
            latency = time.time() - start_time  # Calculate latency
            content = response.choices[0].text.strip()  # Extract response text
            # Create and return a CompletionMessage with the response
            return CompletionMessage(content=content)
        except Exception as e:
            # Log the exception and re-raise for higher-level handling
            logging.error(f"OpenAIClient execution error: {e}")
            raise

class LlamaClient(ToolPlugin):
    """
    Client to interact with the Llama AI API.
    """
    def __init__(self, api_key: str):
        """
        Initialize the LlamaClient with the provided API key.

        Args:
            api_key (str): The API key for Llama.
        """
        self.api_key = api_key  # Store the API key securely
        # Initialize the Llama API client with the API key
        self.client = llama.Client(api_key=self.api_key)
    
    def execute(self, prompt: str) -> CompletionMessage:
        """
        Execute a prompt using the Llama AI model.

        Args:
            prompt (str): The input prompt for the model.

        Returns:
            CompletionMessage: The model's response.
        """
        try:
            start_time = time.time()  # Start latency measurement
            # Make a request to the Llama API to generate a response
            response = self.client.generate(prompt=prompt, max_tokens=150)
            latency = time.time() - start_time  # Calculate latency
            content = response.text.strip()  # Extract response text
            # Create and return a CompletionMessage with the response
            return CompletionMessage(content=content)
        except Exception as e:
            # Log the exception and re-raise for higher-level handling
            logging.error(f"LlamaClient execution error: {e}")
            raise

# ============================================
# 6. Tool Selection Logic
# ============================================

class ToolSelector:
    """
    Service to handle tool selection based on the current mode (manual or automated).

    It selects the appropriate tool considering cost, latency, quality, and CO2 impact.
    """
    def __init__(self, metrics_service: MetricsService):
        """
        Initialize the ToolSelector with a MetricsService.

        Args:
            metrics_service (MetricsService): The service to retrieve tool metrics.
        """
        self.metrics_service = metrics_service  # Store the metrics service
        self.logger = logging.getLogger(__name__)  # Initialize logger
    
    def select_tool_manual(self, available_tools: List[AgentTool]) -> AgentTool:
        """
        Present metrics to the developer and allow manual selection of the tool.

        Args:
            available_tools (List[AgentTool]): List of available tools to choose from.

        Returns:
            AgentTool: The tool selected by the developer.
        """
        # Display tool metrics to the developer
        for tool in available_tools:
            metrics = self.metrics_service.get_metrics(tool)  # Retrieve metrics
            print(f"Tool: {tool.value}")
            print(f"  Cost: ${metrics.cost}")
            print(f"  Latency: {metrics.latency:.2f}s")
            print(f"  Quality: {metrics.quality}")
            print(f"  CO2 Impact: {metrics.co2_impact:.6f}kg\n")
        
        # Prompt the developer to select a tool by typing its name
        while True:
            selected_tool = input("Select a tool by typing its name: ").strip().lower()
            # Attempt to match the input to an available tool
            matched_tools = [tool for tool in available_tools if tool.value.lower() == selected_tool]
            if matched_tools:
                return matched_tools[0]  # Return the matched tool
            else:
                print("Invalid tool name. Please try again.")  # Inform about invalid input
    
    def select_tool_automated(self, available_tools: List[AgentTool]) -> AgentTool:
        """
        Automatically select the best tool based on a heuristic that minimizes cost and CO2 impact
        while maximizing quality.

        Args:
            available_tools (List[AgentTool]): List of available tools to choose from.

        Returns:
            AgentTool: The tool selected automatically.
        """
        best_tool = None  # Initialize the best tool as None
        best_score = float('inf')  # Initialize the best score as infinity (lower is better)
        
        for tool in available_tools:
            metrics = self.metrics_service.get_metrics(tool)  # Retrieve metrics
            # Define a scoring function; customize weights as needed
            # Example: cost and CO2 impact are equally weighted, quality is inversely weighted
            score = metrics.cost + metrics.co2_impact - metrics.quality
            if score < best_score:
                best_score = score  # Update the best score
                best_tool = tool  # Update the best tool
        
        self.log_selection(best_tool)  # Log the automated selection
        return best_tool  # Return the best tool found
    
    def log_selection(self, tool: AgentTool):
        """
        Log the tool selection decision made in automated mode.

        Args:
            tool (AgentTool): The tool that was selected.
        """
        # Log the automated selection at the INFO level
        self.logger.info(f"Automated selection: {tool.value}")
